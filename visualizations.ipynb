{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clusters + centroid plot -> umap: (done on small database)\n",
    "    Make a visualization with umap of the doc vectors in the small database where all vectors from one cluster have the same color. The cluster centroid of each cluster is also colored in the same color but can be differentiated from the rest by line thickness, brightness, ...\n",
    "\n",
    "- relation cosine similarity <> euclidean distance: (done on small database)\n",
    "    Make a lineplot with as horizontal axis the cosine similarity in bins (for instance of 0.1 width [-1.0, -0.9], [-0.9, -0.8], ... , [0.9, 1]) and as vertical axis the corresponding euclidean distance for that bin. The line is the mean of the euclidean distance per bin and there's a confidence interval over it using +- the standard error mean (+-SEM). This can be done with the small database.\n",
    "\n",
    "- effect of top c cluster centroid selection on performance: (done on large database)\n",
    "    Make a barplot with on the horizontal axis the total number of clusters used in the cluster database. For each amount of total clusters, the top-k (e.g. top-10) performance (MAR, MAP - maybe F1 better?) is shown for each number of top c clusters as a percentage of the performance of the baseline (searching over all clusters). This percentage is shown as a colormap from 0%-100%. For instance: when you cluster on 4 total clusters. You can search the top 4 clusters, top 3 clusters, top 2 clusters, top 1 cluster. This will decrease performance with top 4 clusters as baseline since you search over all clusters (exhaustive). You can now do it for total clusters of 2, 3, 4, 5, 10, ... and plot it.\n",
    "\n",
    "- effect of top c cluster centroid selection on time to search: (done on large database)\n",
    "    Make a barplot with on the horizontal axis the total number of clusters used in the cluster database. For each amount of total clusters, the time is measures when you search 100%-75%-50%-25% of available clusters (so 4 bars for each number of total clusters). You would think the search will decrease with more total clusters and less ...% of clusters searched.\n",
    "\n",
    "- Quantization effect on performance (prec/recall or F1?): (done on large database)\n",
    "    Quantize all the vectors from 64bit to 32bit, 16bit, 8bit and check what happens with performance -> line plot\n",
    "\n",
    "- Different sentencetransformers -> better results? (multi-lingual): (done on small database)\n",
    "    Maybe we get better performance with a different sentence transformer model. Take four different models and compare them by performance on the small database but also time to search and encode. Make plots of this. Keep in mind that our dataset is multilingual!\n",
    "\n",
    "- performance difference list np.arrays <> one big np.array: (I will do that)\n",
    "\n",
    "- FAISS implementation: (done on large database)\n",
    "    Facebook AI similarity search package implements all our methods also. \n",
    "        - IVFindex with FlatL2 (=cluster database)\n",
    "        - HSWN (=HSWN database)\n",
    "    -> What is the performance of this implemented? Maybe a better comparison to make with pylucene. (especially search time)\n",
    "\n",
    "- hyperparameters of HSWN plotting: (done on large database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
