{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.evaluation import evaluate_queries, precision_at_k, recall_at_k\n",
    "from src.vector_database import VectorDatabase, ClusterDatabase\n",
    "from src.helpers import process_query_results\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/Users/stighellemans/Desktop/Information_Retrieval/assignments/data\")\n",
    "base_small_doc_path = base_path / \"full_docs_small\"\n",
    "base_large_doc_path = base_path / \"full_docs\"\n",
    "\n",
    "small_docs = {int(re.search(r'\\d+', doc_path.name).group()): doc_path for doc_path in base_small_doc_path.glob(\"*.txt\")}\n",
    "large_docs = {int(re.search(r'\\d+', doc_path.name).group()): doc_path for doc_path in base_large_doc_path.glob(\"*.txt\")}\n",
    "\n",
    "small_queries = pd.read_csv(base_path / \"dev_small_queries - dev_small_queries.csv\", index_col=\"Query number\").to_dict()[\"Query\"]\n",
    "small_query_results = pd.read_csv(base_path / \"dev_query_results_small.csv\", index_col=\"Query_number\")\n",
    "small_query_results = process_query_results(small_query_results)\n",
    "\n",
    "# large_queries = pd.read_csv(base_path / \"dev_queries.tsv\", delimiter=\"\\t\", index_col=\"Query number\").to_dict()[\"Query\"]\n",
    "# large_query_results = pd.read_csv(base_path / \"dev_query_results.csv\", index_col=\"Query_number\")\n",
    "# large_query_results = process_query_results(large_queries, large_query_results)\n",
    "\n",
    "# test_queries = pd.read_csv(base_path / \"queries.csv\", delimiter=\"\\t\", index_col=\"Query number\").to_dict()[\"Query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VectorDatabase(model)\n",
    "db.load_database(\"./results/small_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 ms ± 40.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "evaluate_queries(small_queries, small_query_results, k_values=[1, 3, 5, 10], database=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_db = ClusterDatabase(model)\n",
    "cluster_db.load_database(\"./results/large_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.060675777, 1.370638, 1.0, 0.99999994)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 = db.doc_vectors[1]\n",
    "vector2 = db.doc_vectors[2]\n",
    "\n",
    "np.dot(vector1, vector2), np.linalg.norm(vector1 - vector2), np.linalg.norm(vector1), np.linalg.norm(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 ms ± 26.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "evaluate_queries(small_queries, small_query_results, k_values=[1, 3, 5, 10], database=cluster_db, top_c=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1557, 248)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vectors = np.array(db.encoder.encode(list(small_queries.values())))\n",
    "sims = np.dot(db.doc_vectors, query_vectors.T)\n",
    "\n",
    "top_k_indices = np.argsort(-sims, axis=0)[:k]\n",
    "\n",
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(db.doc_vectors)\n",
    "cluster_labels = kmeans.labels_\n",
    "doc_vectors_split = [\n",
    "            db.doc_vectors[cluster_labels == i] for i in range(n_clusters)\n",
    "        ]\n",
    "doc_ids_split = [\n",
    "            np.where(cluster_labels == i)[0] for i in range(n_clusters)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  52   45  271  326 1112   56 1358  894 1082 1226]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[9 9 2 9 2 9 2 9 1 9]\n",
      "[ 0.0225224   0.01120792  0.02444224  0.00127908  0.00766633 -0.00567381\n",
      "  0.01702095  0.01442289  0.00767733  0.06765988]\n",
      "[9 2 0 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'Precision': 0.4435483870967742, 'Recall': 0.4375},\n",
       " 3: {'Precision': 0.1948924731182796, 'Recall': 0.5725806451612904},\n",
       " 5: {'Precision': 0.12580645161290321, 'Recall': 0.6129032258064516},\n",
       " 10: {'Precision': 0.06774193548387096, 'Recall': 0.6612903225806451}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = list(small_queries.values())\n",
    "top_c = 4\n",
    "top_k = 10\n",
    "\n",
    "query_vectors = np.array(db.encoder.encode(queries))\n",
    "sims = np.dot(db.doc_vectors, query_vectors.T)\n",
    "top_k_indices = np.argsort(-sims, axis=0)[:top_k]\n",
    "\n",
    "\n",
    "print(top_k_indices[:, 0])\n",
    "print(np.unique(kmeans.labels_))\n",
    "print(kmeans.labels_[top_k_indices[:, 0]])\n",
    "\n",
    "\n",
    "# Compute similarity of cluster centers to queries\n",
    "sims = np.dot(kmeans.cluster_centers_, query_vectors.T)\n",
    "query_top_k_clusters = np.argsort(-sims, axis=0)[:top_c]\n",
    "\n",
    "print(sims[:, 0])\n",
    "print(query_top_k_clusters[: , 0])\n",
    "\n",
    "retrieved_doc_ids = np.empty((top_c * top_k, len(small_queries)))\n",
    "retrieved_sims = np.empty((top_c * top_k, len(small_queries)))\n",
    "\n",
    "for c, (doc_vector_cluster, doc_ids_cluster) in enumerate(zip(doc_vectors_split, doc_ids_split)):\n",
    "    for i in range(top_c):\n",
    "        queries_per_cluster = np.where(query_top_k_clusters[i, :] == c)[0]\n",
    "\n",
    "        if queries_per_cluster.size == 0:\n",
    "            continue\n",
    "\n",
    "        sims = np.dot(doc_vector_cluster, query_vectors[queries_per_cluster].T)\n",
    "\n",
    "        top_k_cluster_indices = np.argsort(-sims, axis=0)[:top_k]\n",
    "        top_k_indices = doc_ids_cluster[top_k_cluster_indices]\n",
    "        top_k_sims = sims[top_k_cluster_indices, np.arange(len(queries_per_cluster))]\n",
    "\n",
    "        retrieved_doc_ids[i * top_k:(i + 1) * top_k, queries_per_cluster] = top_k_indices\n",
    "        retrieved_sims[i * top_k:(i + 1) * top_k, queries_per_cluster] = top_k_sims\n",
    "\n",
    "top_k_indices = np.argsort(-retrieved_sims, axis=0)[:top_k]\n",
    "retrieved_doc_ids = retrieved_doc_ids[top_k_indices, np.arange(len(small_queries))]\n",
    "\n",
    "vectorized_translate = np.vectorize(db.translate_id)\n",
    "top_k_indices = vectorized_translate(retrieved_doc_ids)\n",
    "\n",
    "k_values = [1, 3, 5, 10]\n",
    "query_results = small_query_results\n",
    "\n",
    "output = {}\n",
    "for k in k_values:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i, query_id in enumerate(list(small_queries.keys())):\n",
    "        retrieved = top_k_indices[:k, i]\n",
    "        relevant = query_results[query_id]\n",
    "        precisions.append(precision_at_k(list(relevant), list(retrieved), k))\n",
    "        recalls.append(recall_at_k(list(relevant), list(retrieved), k))\n",
    "    precisions = np.mean(precisions)\n",
    "    recalls = np.mean(recalls)\n",
    "\n",
    "    output[k] = {\"Precision\": precisions, \"Recall\": recalls}\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'Precision': 0.0, 'Recall': 0.0},\n",
       " 3: {'Precision': 0.0, 'Recall': 0.0},\n",
       " 5: {'Precision': 0.0, 'Recall': 0.0},\n",
       " 10: {'Precision': 0.0, 'Recall': 0.0}}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
